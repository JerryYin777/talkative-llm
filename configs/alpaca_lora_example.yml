framework: "alpaca-lora"

mode: "LlamaForCausalLM"
model: "decapoda-research/llama-7b-hf"
device: "cuda"                           # 'cpu' or 'cuda'
lora_weights: "tloen/alpaca-lora-7b"
load_8bit: false                         # if device is 'cpu', then load_8bit must be true
batch_size: 20
skip_special_tokens: true

params:
  max_new_tokens: 128
  early_stopping: true
  num_beams: 3
  use_cache: true
  temperature: 0.1
  top_p: 0.75
  top_k: 40
  num_return_sequences: 1